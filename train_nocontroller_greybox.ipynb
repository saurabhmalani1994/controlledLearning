{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import utils\n",
    "import khammash_repro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Training Data\n",
    "\n",
    "path = \"/home/smalani/controlledLearning/training_data\"\n",
    "training_L = np.load(path + \"/training_L.npy\").astype(np.float32)\n",
    "training_sp = np.load(path + \"/training_sp.npy\").astype(np.float32)\n",
    "training_t = np.load(path + \"/training_t.npy\").astype(np.float32)\n",
    "training_y = np.load(path + \"/training_y.npy\").astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, training_t, training_y, training_L, time_forecast=1):\n",
    "        t0, t1, L0, L1, y0, y1 = self.split_data(training_t, training_L, training_y, time_forecast)\n",
    "        self.t0 = t0\n",
    "        self.t1 = t1\n",
    "        self.L0 = L0\n",
    "        self.L1 = L1\n",
    "        self.y0 = y0\n",
    "        self.y1 = y1\n",
    "\n",
    "    def split_data(self, t, L, y, time_forecast):\n",
    "        total_time_length = t.shape[1]\n",
    "\n",
    "        t0, t1, L0, L1, y0, y1 = [], [], [], [], [], []\n",
    "\n",
    "        for j in range(t.shape[0]):\n",
    "            time_forecast = np.clip(time_forecast, 1, total_time_length-1)\n",
    "            for i in range(total_time_length - time_forecast):\n",
    "                t0.append(t[j, i])\n",
    "\n",
    "\n",
    "                t1_add, L0_add, L1_add, y1_add, y0_add = [], [], [], [], []\n",
    "                for k in range(time_forecast):\n",
    "                    t1_add.append(t[j, i + k+1])\n",
    "                    L0_add.append(L[j, i + k])\n",
    "                    y0_add.append(y[j, i+k])\n",
    "                    L1_add.append(L[j, i + k+1])\n",
    "                    y1_add.append(y[j, i + k+1])\n",
    "\n",
    "                t1.append(t1_add)\n",
    "                L1.append(L1_add)\n",
    "                L0.append(L0_add)\n",
    "                y1.append(y1_add)\n",
    "                y0.append(y0_add)\n",
    "\n",
    "\n",
    "        t0 = np.array(t0)\n",
    "        t1 = np.array(t1)\n",
    "        L0 = np.array(L0)\n",
    "        L1 = np.array(L1)\n",
    "        y0 = np.array(y0)\n",
    "        y1 = np.array(y1)\n",
    "\n",
    "        return t0, t1, L0, L1, y0, y1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.t0.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t0 = torch.tensor(self.t0[idx]).unsqueeze(0)\n",
    "        t1 = torch.tensor(self.t1[idx])#.unsqueeze(0)\n",
    "        L0 = torch.tensor(self.L0[idx])#.unsqueeze(0)\n",
    "        L1 = torch.tensor(self.L1[idx]).unsqueeze(0)\n",
    "        y0 = torch.tensor(self.y0[idx])\n",
    "        y1 = torch.tensor(self.y1[idx])\n",
    "\n",
    "        return t0, t1, L0, L1, y0, y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, device, max_step, box='black'):\n",
    "        super(Model, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # self.hidden_network = torch.nn.Sequential(\n",
    "        #     torch.nn.Linear(5, 64),\n",
    "        #     torch.nn.ReLU(),\n",
    "        #     torch.nn.Linear(64, 64),\n",
    "        #     torch.nn.ReLU(),\n",
    "        #     torch.nn.Linear(64, 3)\n",
    "        # )\n",
    "        self.hidden_fc1 = torch.nn.Linear(5, 64)\n",
    "        self.hidden_fc2 = torch.nn.Linear(64, 64)\n",
    "        self.hidden_fc3 = torch.nn.Linear(64, 3)\n",
    "\n",
    "        self.growth_network = torch.nn.Sequential(\n",
    "            torch.nn.Linear(4, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "        self.device = device\n",
    "        self.max_step = max_step\n",
    "\n",
    "        self.initial_vector = None\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def hidden_network(self, x):\n",
    "        x = self.hidden_fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.hidden_fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.hidden_fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def initialize_initial_conditions(self, y0):\n",
    "        self.initial_vector = torch.zeros((y0.shape[0], 3), device=self.device, requires_grad=True)\n",
    "        self.lambda_c = torch.tensor(1.0, device=self.device, requires_grad=True)\n",
    "    \n",
    "    def ODEs(self, hidden, phi, L0):\n",
    "        hidden_derivative = self.hidden_network(torch.cat((hidden, phi, L0/800), dim=1))\n",
    "        lambda_p = self.growth_network(torch.cat((hidden, phi), dim=1))\n",
    "        dphi = (lambda_p - self.lambda_c) * phi * (1-phi)\n",
    "\n",
    "        dydt = torch.cat((hidden_derivative, dphi), dim=1)\n",
    "\n",
    "        return dydt\n",
    "    \n",
    "    def RK4(self, hidden, phi, L0, dt):\n",
    "        k1 = self.ODEs(hidden, phi, L0)\n",
    "        k2 = self.ODEs(hidden + dt/2 * k1[...,:3], phi + dt/2 * k1[...,[3]], L0)\n",
    "        k3 = self.ODEs(hidden + dt/2 * k2[...,:3], phi + dt/2 * k2[...,[3]], L0)\n",
    "        k4 = self.ODEs(hidden + dt * k3[...,:3], phi + dt * k3[...,[3]], L0)\n",
    "\n",
    "        hidden_next = hidden + dt/6 * (k1[...,:3] + 2*k2[...,:3] + 2*k3[...,:3] + k4[...,:3])\n",
    "        phi_next = phi + dt/6 * (k1[...,[3]] + 2*k2[...,[3]] + 2*k3[...,[3]] + k4[...,[3]])\n",
    "\n",
    "        return hidden_next, phi_next\n",
    "\n",
    "    def move_to_device(self, t0, t1, L0, y0):\n",
    "        t0 = t0.to(self.device)\n",
    "        t1 = t1.to(self.device)\n",
    "        L0 = L0.to(self.device)\n",
    "        y0 = y0.to(self.device)\n",
    "\n",
    "        return t0, t1, L0, y0\n",
    "    \n",
    "    def forward(self, t0, t1, L0, y0, autoreg=1):\n",
    "        if self.initial_vector is None:\n",
    "            self.initialize_initial_conditions(y0)\n",
    "        autoreg = np.clip(autoreg, 0, 1)\n",
    "        t0, t1, L0, y0 = self.move_to_device(t0, t1, L0, y0)\n",
    "        past_time = t0[:, [0]]\n",
    "\n",
    "        y_out = []\n",
    "        hidden_out = []\n",
    "\n",
    "        y_out.append(y0[:, 0])\n",
    "        hidden_out.append(self.initial_vector)\n",
    "\n",
    "        for i in range(t1.shape[1]):\n",
    "            dt = t1[:, [i]] - past_time\n",
    "            past_time = t1[:, [i]]\n",
    "            dt_split = torch.rand(dt.shape).to(self.device)\n",
    "            dt1 = dt * dt_split\n",
    "            dt2 = dt - dt1\n",
    "\n",
    "            L_in = L0[:, [i]]\n",
    "\n",
    "            hidden_in = hidden_out[-1]\n",
    "            if int(autoreg * t1.shape[1]) == 0 or i % int(autoreg * t1.shape[1]) == 0:\n",
    "                y_in = y0[:, i]\n",
    "            else:\n",
    "                y_in = y_out[-1]\n",
    "\n",
    "            hidden_in, y_in = self.RK4(hidden_in, y_in, L_in, dt1)\n",
    "            hidden_sol, y_sol = self.RK4(hidden_in, y_in, L_in, dt2)\n",
    "\n",
    "            y_out.append(y_sol)\n",
    "            hidden_out.append(hidden_sol)\n",
    "        y_out = torch.stack(y_out[1:], dim=1)\n",
    "        return hidden_out, y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "time_forecast = 10000\n",
    "train_dataset = Dataset(training_t[:3,:], training_y[:3,:,:], training_L[:3,:], time_forecast=time_forecast)\n",
    "val_dataset = Dataset(training_t[[3],:], training_y[[3],:,:], training_L[[3],:], time_forecast=time_forecast)\n",
    "\n",
    "\n",
    "# Min/Max Normalization\n",
    "\n",
    "\n",
    "# Train Validation Split\n",
    "# train_size = int(1.0 * len(dataset))\n",
    "# val_size = len(dataset) - train_size\n",
    "# train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Define the dataloader\n",
    "batch_size = 10000\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Create the model\n",
    "input_size = 5\n",
    "hidden_size = 64\n",
    "output_size = 4\n",
    "\n",
    "max_step = 0.04384\n",
    "\n",
    "model = Model(input_size, hidden_size, output_size, device, max_step).to(device)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr= 1e-2)\n",
    "\n",
    "# Define the lr scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=50, verbose=True)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000, eta_min=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training loop\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, lr_scheduler=None, autoreg=0):\n",
    "    size = len(dataloader.dataset)\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for batch, (t0, t1, L0, L1, y0, y1) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        _, pred = model(t0, t1, L0, y0[...,[3]], autoreg)\n",
    "\n",
    "        # Scale the predictions\n",
    "        y1 = y1[...,[3]].to(device)\n",
    "        loss_ar = loss_fn(pred, y1)\n",
    "\n",
    "        # Compute prediction and loss\n",
    "        _, pred = model(t0, t1, L0, y0[...,[3]], 0)\n",
    "\n",
    "        # Scale the predictions\n",
    "        loss_tf = loss_fn(pred, y1)\n",
    "        loss = torch.log(loss_ar) + torch.log(loss_tf)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # if batch % 100 == 0:\n",
    "        #     loss, current = loss.item(), batch * len(t0)\n",
    "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    \n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step(train_loss)\n",
    "\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    return train_loss , lr\n",
    "\n",
    "\n",
    "def val_loop(dataloader, model, loss_fn, autoreg=0):\n",
    "    size = len(dataloader.dataset)\n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for t0, t1, L0, L1, y0, y1 in dataloader:\n",
    "        # Compute prediction and loss\n",
    "            _, pred = model(t0, t1, L0, y0[...,[3]], autoreg)\n",
    "\n",
    "            # Scale the predictions\n",
    "            y1 = y1[...,[3]].to(device)\n",
    "            \n",
    "            val_loss += torch.log(loss_fn(pred, y1)).item()\n",
    "    \n",
    "    return val_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a663bbd4bbb4a6da443e8faa9329123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/smalani/controlledLearning/train_nocontroller_greybox.ipynb Cell 7\u001b[0m in \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdella-gpu.princeton.edu/home/smalani/controlledLearning/train_nocontroller_greybox.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m         \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m optimizer\u001b[39m.\u001b[39mparam_groups:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdella-gpu.princeton.edu/home/smalani/controlledLearning/train_nocontroller_greybox.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m             g[\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1e-2\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdella-gpu.princeton.edu/home/smalani/controlledLearning/train_nocontroller_greybox.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m train_loss, lr \u001b[39m=\u001b[39m train_loop(train_dataloader, model, loss_fn, optimizer, lr_scheduler, autoreg)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdella-gpu.princeton.edu/home/smalani/controlledLearning/train_nocontroller_greybox.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# val_loss = val_loop(val_dataloader, model, loss_fn, 1)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdella-gpu.princeton.edu/home/smalani/controlledLearning/train_nocontroller_greybox.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m train_loss_list\u001b[39m.\u001b[39mappend(train_loss)\n",
      "\u001b[1;32m/home/smalani/controlledLearning/train_nocontroller_greybox.ipynb Cell 7\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdella-gpu.princeton.edu/home/smalani/controlledLearning/train_nocontroller_greybox.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m _, pred \u001b[39m=\u001b[39m model(t0, t1, L0, y0[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m,[\u001b[39m3\u001b[39m]], \u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdella-gpu.princeton.edu/home/smalani/controlledLearning/train_nocontroller_greybox.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Scale the predictions\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdella-gpu.princeton.edu/home/smalani/controlledLearning/train_nocontroller_greybox.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m y1 \u001b[39m=\u001b[39m y1[\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m,[\u001b[39m3\u001b[39;49m]]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdella-gpu.princeton.edu/home/smalani/controlledLearning/train_nocontroller_greybox.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m loss_tf \u001b[39m=\u001b[39m loss_fn(pred, y1)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdella-gpu.princeton.edu/home/smalani/controlledLearning/train_nocontroller_greybox.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlog(loss_ar) \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mlog(loss_tf)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "tf_epochs = 100\n",
    "transition_epochs = 100\n",
    "autoreg_epochs = 1000\n",
    "epochs = tf_epochs + transition_epochs + autoreg_epochs\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "lr_list = []\n",
    "\n",
    "pbar = tqdm(range(epochs))\n",
    "lr_scheduler = None\n",
    "\n",
    "for t in pbar:\n",
    "    if t < tf_epochs:\n",
    "        autoreg = 0\n",
    "    elif t < tf_epochs + transition_epochs:\n",
    "        autoreg = (t - tf_epochs) / transition_epochs\n",
    "    else:\n",
    "        autoreg = 1\n",
    "        if lr_scheduler is None:\n",
    "            lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=50, verbose=True)\n",
    "            # lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000, eta_min=1e-6)\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] = 1e-2\n",
    "\n",
    "    train_loss, lr = train_loop(train_dataloader, model, loss_fn, optimizer, lr_scheduler, autoreg)\n",
    "    # val_loss = val_loop(val_dataloader, model, loss_fn, 1)\n",
    "\n",
    "    train_loss_list.append(train_loss)\n",
    "    # val_loss_list.append(val_loss)\n",
    "    lr_list.append(lr)\n",
    "\n",
    "    pbar.set_description(f\"Epoch {t+1}\")\n",
    "    pbar.set_postfix(train_loss=train_loss, lr=lr, autoreg=autoreg)#, val_loss=val_loss)\n",
    "\n",
    "    if t % 100 == 0:\n",
    "        path = \"/home/smalani/controlledLearning/trained_models/\"\n",
    "        torch.save(model, path + \"modeltrain_nocontroller_longtraj.pt\")\n",
    "        \n",
    "    # pbar.set_postfix(train_loss=train_loss, val_loss=val_loss, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the loss\n",
    "\n",
    "# fig = plt.figure(figsize=(5, 4))\n",
    "# ax = fig.add_subplot(111)\n",
    "# ax.semilogy(train_loss_list, label='Train Loss')\n",
    "# ax.semilogy(val_loss_list, label='Validation Loss')\n",
    "# ax.set_xlabel('Epochs')\n",
    "# ax.set_ylabel('Loss')\n",
    "# ax.set_title('Loss vs Epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Performance on training data\n",
    "\n",
    "def learned_model_ode(t, y0, L0):\n",
    "    L0 = torch.tensor(L0).to(device)\n",
    "    y0 = torch.tensor(y0).to(device)\n",
    "    if len(y0.shape) == 1:\n",
    "        y0 = y0.unsqueeze(0)\n",
    "    while len(y0.shape) > len(L0.shape):\n",
    "        L0 = L0.unsqueeze(-1)\n",
    "    dydt = model.ODEs(y0, L0)\n",
    "\n",
    "    return dydt.cpu().detach().numpy().squeeze()\n",
    "    \n",
    "\n",
    "true_dydt = khammash_repro.ode_fun(0, dataset.y0.T, dataset.L0[:,0]).T * 60\n",
    "learned_dydt = learned_model_ode(0, dataset.y0, dataset.L0[:,0])\n",
    "\n",
    "fig = plt.figure(figsize=(5, 4))\n",
    "ax = fig.add_subplot(221)\n",
    "ax.scatter(true_dydt[:,0], learned_dydt[:,0], s=1)\n",
    "ax.plot([np.min(true_dydt[:,0]), np.max(true_dydt[:,0])], [np.min(true_dydt[:,0]), np.max(true_dydt[:,0])], 'k--')\n",
    "\n",
    "ax = fig.add_subplot(222)\n",
    "ax.scatter(true_dydt[:,1], learned_dydt[:,1], s=1)\n",
    "ax.plot([np.min(true_dydt[:,1]), np.max(true_dydt[:,1])], [np.min(true_dydt[:,1]), np.max(true_dydt[:,1])], 'k--')\n",
    "\n",
    "ax = fig.add_subplot(223)\n",
    "ax.scatter(true_dydt[:,2], learned_dydt[:,2], s=1)\n",
    "ax.plot([np.min(true_dydt[:,2]), np.max(true_dydt[:,2])], [np.min(true_dydt[:,2]), np.max(true_dydt[:,2])], 'k--')\n",
    "\n",
    "ax = fig.add_subplot(224)\n",
    "ax.scatter(true_dydt[:,3], learned_dydt[:,3], s=1)\n",
    "ax.plot([np.min(true_dydt[:,3]), np.max(true_dydt[:,3])], [np.min(true_dydt[:,3]), np.max(true_dydt[:,3])], 'k--')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = torch.tensor(dataset.t0).to(device).unsqueeze(-1)\n",
    "t1 = torch.tensor(dataset.t1).to(device)#.unsqueeze(-1)\n",
    "L0 = torch.tensor(dataset.L0).to(device)#.unsqueeze(-1)\n",
    "y0 = torch.tensor(dataset.y0).to(device)\n",
    "\n",
    "y1_pred = model.forward(t0, t1, L0, y0).cpu().detach().numpy()\n",
    "\n",
    "fig = plt.figure(figsize=(5, 4))\n",
    "ax = fig.add_subplot(221)\n",
    "ax.scatter(dataset.y1[:,0], y1_pred[:,0], s=1)\n",
    "ax.plot([np.min([dataset.y1[:,0]]), np.max([dataset.y1[:,0]])], [np.min([dataset.y1[:,0]]), np.max([dataset.y1[:,0]])], 'k--')\n",
    "\n",
    "ax = fig.add_subplot(222)\n",
    "ax.scatter(dataset.y1[:,1], y1_pred[:,1], s=1)\n",
    "ax.plot([np.min([dataset.y1[:,1]]), np.max([dataset.y1[:,1]])], [np.min([dataset.y1[:,1]]), np.max([dataset.y1[:,1]])], 'k--')\n",
    "\n",
    "ax = fig.add_subplot(223)\n",
    "ax.scatter(dataset.y1[:,2], y1_pred[:,2], s=1)\n",
    "ax.plot([np.min([dataset.y1[:,2]]), np.max([dataset.y1[:,2]])], [np.min([dataset.y1[:,2]]), np.max([dataset.y1[:,2]])], 'k--')\n",
    "\n",
    "ax = fig.add_subplot(224)\n",
    "ax.scatter(dataset.y1[:,3], y1_pred[:,3], s=1)\n",
    "ax.plot([np.min([dataset.y1[:,3]]), np.max([dataset.y1[:,3]])], [np.min([dataset.y1[:,3]]), np.max([dataset.y1[:,3]])], 'k--')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"/home/smalani/controlledLearning/trained_models/\"\n",
    "# torch.save(model, path + \"modeltrain_nocontroller.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model and test\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "path = \"/home/smalani/controlledLearning/trained_models/\"\n",
    "model = torch.load(path + \"modeltrain_nocontroller.pt\", map_location=\"cpu\")\n",
    "model.device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "def learned_model_ode(t, y0, L0):\n",
    "    L0 = torch.tensor(L0).to(device)\n",
    "    y0 = torch.tensor(y0).to(device)\n",
    "    if len(y0.shape) == 1:\n",
    "        y0 = y0.unsqueeze(0)\n",
    "    while len(y0.shape) > len(L0.shape):\n",
    "        L0 = L0.unsqueeze(-1)\n",
    "    dydt = model.ODEs(y0, L0)\n",
    "\n",
    "    return dydt.cpu().detach().numpy().squeeze()\n",
    "\n",
    "x_init = khammash_repro.get_init_cond(L0=200)\n",
    "x_init[-1] = 0.4\n",
    "x_init = dataset.y0[0,:]\n",
    "L0 = 0\n",
    "t_span = [0, 100]\n",
    "t_eval = np.linspace(t_span[0], t_span[1], 1000)\n",
    "\n",
    "sol_true = solve_ivp(khammash_repro.ode_fun, t_span, x_init, args=(L0,), t_eval=t_eval, method='BDF', rtol=1e-10, atol=1e-10, first_step=1e-10)\n",
    "sol_learned = solve_ivp(learned_model_ode, t_span, x_init, args=(L0,), t_eval=t_eval, method='BDF', rtol=1e-10, atol=1e-10, first_step=1e-10)\n",
    "\n",
    "print(learned_model_ode(0, x_init, L0))\n",
    "print(khammash_repro.ode_fun(0, x_init, L0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 4))\n",
    "for i in range(4):\n",
    "    ax = fig.add_subplot(2,2,i+1)\n",
    "    ax.plot(sol_true.t, sol_true.y[i,:], 'k-', label='True')\n",
    "    ax.plot(sol_learned.t, sol_learned.y[i,:], 'r--', label='Learned')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Species {}'.format(i+1))\n",
    "    ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
