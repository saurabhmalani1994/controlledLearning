{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import utils\n",
    "import khammash_repro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Training Data\n",
    "\n",
    "path = \"/home/smalani/controlledLearning/training_data\"\n",
    "training_L = np.load(path + \"/training_L.npy\")\n",
    "training_sp = np.load(path + \"/training_sp.npy\")\n",
    "training_t = np.load(path + \"/training_t.npy\")\n",
    "training_y = np.load(path + \"/training_y.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(training_sp >= 0.8) / training_L.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, training_t, training_y, training_L, training_sp, time_forecast=1):\n",
    "        t0, t1, L0, L1, y0, y1, sp = self.split_data(training_t, training_L, training_y, training_sp, time_forecast)\n",
    "        self.t0 = t0\n",
    "        self.t1 = t1\n",
    "        self.L0 = L0\n",
    "        self.L1 = L1\n",
    "        self.y0 = y0\n",
    "        self.y1 = y1\n",
    "        self.sp = sp\n",
    "\n",
    "    def split_data(self, t, L, y, sp, time_forecast):\n",
    "        total_time_length = t.shape[1]\n",
    "\n",
    "        t0, t1, L0, L1, y0, y1, sp_out = [], [], [], [], [], [], []\n",
    "\n",
    "        for j in range(t.shape[0]):\n",
    "            time_forecast = np.clip(time_forecast, 1, total_time_length-1)\n",
    "            for i in range(total_time_length - time_forecast):\n",
    "                t0.append(t[j, i])\n",
    "\n",
    "                t1_add, L0_add, L1_add, y1_add, y0_add, sp_add = [], [], [], [], [], []\n",
    "                for k in range(time_forecast):\n",
    "                    t1_add.append(t[j, i + k+1])\n",
    "                    L0_add.append(L[j, i + k])\n",
    "                    y0_add.append(y[j, i+k])\n",
    "                    L1_add.append(L[j, i + k+1])\n",
    "                    y1_add.append(y[j, i + k+1])\n",
    "                    sp_add.append(sp[j, i + k])\n",
    "\n",
    "                t1.append(t1_add)\n",
    "                L1.append(L1_add)\n",
    "                L0.append(L0_add)\n",
    "                y1.append(y1_add)\n",
    "                y0.append(y0_add)\n",
    "                sp_out.append(sp_add)\n",
    "\n",
    "        t0 = np.array(t0)\n",
    "        t1 = np.array(t1)\n",
    "        L0 = np.array(L0)\n",
    "        L1 = np.array(L1)\n",
    "        y0 = np.array(y0)\n",
    "        y1 = np.array(y1)\n",
    "        sp_out = np.array(sp_out)\n",
    "\n",
    "        return t0, t1, L0, L1, y0, y1, sp_out\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.t0.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t0 = torch.tensor(self.t0[idx]).unsqueeze(0)\n",
    "        t1 = torch.tensor(self.t1[idx])#.unsqueeze(0)\n",
    "        L0 = torch.tensor(self.L0[idx])#.unsqueeze(0)\n",
    "        L1 = torch.tensor(self.L1[idx]).unsqueeze(-1)\n",
    "        y0 = torch.tensor(self.y0[idx])\n",
    "        y1 = torch.tensor(self.y1[idx])\n",
    "        sp = torch.tensor(self.sp[idx])\n",
    "\n",
    "        return t0, t1, L0, L1, y0, y1, sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, device, max_step, pid_pars):\n",
    "        super(Model, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.input_scale = torch.tensor([1000, 100, 100, 1, 800]).unsqueeze(0).to(device)\n",
    "        self.ode_scale = torch.tensor([100, 100, 100, 1]).unsqueeze(0).to(device)\n",
    "\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "        self.device = device\n",
    "        self.max_step = max_step\n",
    "\n",
    "        Kp, Ki, Kd, Kbc = pid_pars\n",
    "        self.Kp = torch.tensor(Kp).to(device)\n",
    "        self.Ki = torch.tensor(Ki).to(device)\n",
    "        self.Kd = torch.tensor(Kd).to(device)\n",
    "        self.Kbc = torch.tensor(Kbc).to(device)\n",
    "\n",
    "        self.prev_error = None\n",
    "\n",
    "    def scaledown_input(self, x):\n",
    "        xmin = torch.tensor([800, 11, 50, 0, 0]).unsqueeze(0).to(self.device)\n",
    "        xmax = torch.tensor([1837, 259, 510, 1, 800]).unsqueeze(0).to(self.device)\n",
    "        x = (x - xmin) / (xmax - xmin)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def scaleup_output(self, x):\n",
    "        xmin = torch.tensor([-220, -412, -218, -0.2]).unsqueeze(0).to(self.device)\n",
    "        xmax = torch.tensor([640, 461, 300, 0.042]).unsqueeze(0).to(self.device)\n",
    "        x = x * (xmax - xmin) + xmin\n",
    "\n",
    "        return x\n",
    "\n",
    "    def network(self, y0, L0):\n",
    "        x = torch.cat((y0, L0), dim=1)\n",
    "        x = self.scaledown_input(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = torch.clamp(x, min=-1, max=2)\n",
    "\n",
    "        x = self.scaleup_output(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def ODEs(self, y0, L0):\n",
    "        dydt = self.network(y0, L0)# * self.ode_scale\n",
    "\n",
    "        return dydt\n",
    "    \n",
    "    def RK4(self, y0, L0, dt):\n",
    "        k1 = self.ODEs(y0, L0)\n",
    "        k2 = self.ODEs(y0 + dt/2 * k1, L0)\n",
    "        k3 = self.ODEs(y0 + dt/2 * k2, L0)\n",
    "        k4 = self.ODEs(y0 + dt * k3, L0)\n",
    "\n",
    "        y1 = y0 + dt/6 * (k1 + 2*k2 + 2*k3 + k4)\n",
    "\n",
    "        return y1\n",
    "\n",
    "    def move_to_device(self, t0, t1, L0, y0):\n",
    "        t0 = t0.to(self.device)\n",
    "        t1 = t1.to(self.device)\n",
    "        L0 = L0.to(self.device)\n",
    "        y0 = y0.to(self.device)\n",
    "\n",
    "        return t0, t1, L0, y0\n",
    "    \n",
    "    def controller_update(self, y0, sp, prev_error, error_accum, dt):\n",
    "        phi = y0[...,[3]]\n",
    "        error = sp - phi\n",
    "\n",
    "        error_accum = error_accum + error * dt * 60\n",
    "\n",
    "        prop_contribution = self.Kp * error\n",
    "        int_contribution = self.Ki * error_accum\n",
    "        if prev_error is None:\n",
    "            der_contribution = 0\n",
    "        else:\n",
    "            der_contribution = self.Kd * (error - prev_error) / (dt * 60)\n",
    "        prev_error = error\n",
    "\n",
    "        u_compute = torch.round(prop_contribution + int_contribution + der_contribution)\n",
    "\n",
    "        u = torch.clamp(u_compute, min=0, max=800)\n",
    "\n",
    "        bc = u - u_compute\n",
    "        error_accum = error_accum + bc * self.Kbc\n",
    "\n",
    "        return u, prev_error, error_accum\n",
    "\n",
    "    \n",
    "    def forward(self, t0, t1, L0, sp, y0, autoreg=1, L_autoreg=0, control_time=0.5):\n",
    "        autoreg = np.clip(autoreg, 0, 1)\n",
    "        t0, t1, L0, y0 = self.move_to_device(t0, t1, L0, y0)\n",
    "        past_time = t0[:, [0]]\n",
    "        y_out = []\n",
    "        y_out.append(y0[:, 0])\n",
    "\n",
    "        L_out = []\n",
    "        L_out.append(L0[:, 0])\n",
    "\n",
    "        prev_error = None\n",
    "        error_accum = torch.zeros(y0.shape[0], 1).to(self.device)\n",
    "        true_error_accum = torch.zeros(y0.shape[0], 1).to(self.device)\n",
    "\n",
    "        dt = t1[0, [0]] - t0[0, [0]]\n",
    "\n",
    "        control_iterations = int(control_time / dt)\n",
    "\n",
    "        for i in range(t1.shape[1]):\n",
    "            # dt = t1[:, [i]] - past_time\n",
    "            # past_time = t1[:, [i]]\n",
    "            dt_split = torch.rand(dt.shape).to(self.device)\n",
    "            dt1 = dt * dt_split\n",
    "            dt2 = dt - dt1\n",
    "\n",
    "            sp_in = sp[:, [i]]\n",
    "\n",
    "            true_error_accum = true_error_accum + (sp_in - y_out[-1][...,[3]]) * dt * 60\n",
    "\n",
    "            mask = (torch.rand(y0[:, i].shape).to(self.device) < autoreg)*1\n",
    "            y_in = y0[:, i] * (1 - mask) + y_out[-1] * mask\n",
    "\n",
    "            if i % control_iterations == 0:\n",
    "                L_sol, prev_error, error_accum = self.controller_update(y_in, sp_in, prev_error, error_accum, control_time)\n",
    "            else:\n",
    "                L_sol = L_out[-1]\n",
    "\n",
    "            L_mask = (torch.rand(L0[:, [i]].shape).to(self.device) <= L_autoreg)*1\n",
    "            L_in = L0[:, [i]] * (1 - L_mask) + L_sol * L_mask\n",
    "            error_accum = true_error_accum * (1 - L_mask) + error_accum * L_mask\n",
    "            # L_in = L_sol * L_autoreg + L0[:, [i]] * (1 - L_autoreg)\n",
    "\n",
    "            # L_in = L_sol\n",
    "            # if int(autoreg * t1.shape[1]) == 0 or i % int(autoreg * t1.shape[1]) == 0:\n",
    "            #     y_in = y0[:, i]\n",
    "            # else:\n",
    "            #     y_in = y_out[-1]\n",
    "            y_in = self.RK4(y_in, L_in, dt1)\n",
    "            y_sol = self.RK4(y_in, L_in, dt2)\n",
    "\n",
    "            y_out.append(y_sol)\n",
    "            L_out.append(L_sol)\n",
    "        y_out = torch.stack(y_out[1:], dim=1)\n",
    "        L_out = torch.stack(L_out[1:], dim=1)\n",
    "        return y_out, L_out.squeeze(-1)\n",
    "    \n",
    "    def forward_backup(self, t0, t1, L0, y0):\n",
    "        t0, t1, L0, y0 = self.move_to_device(t0, t1, L0, y0)\n",
    "\n",
    "        dt = t1 - t0\n",
    "        dt_split = torch.rand(dt.shape).to(self.device)\n",
    "        dt1 = dt * dt_split\n",
    "        dt2 = dt - dt1\n",
    "\n",
    "        y_int = self.RK4(y0, L0, dt1)\n",
    "        y_pred = self.RK4(y_int, L0, dt2)\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def forward_backup2(self, t0, t1, L0, y0):  \n",
    "        t0, t1, L0, y0 = self.move_to_device(t0, t1, L0, y0)\n",
    "\n",
    "        dt = t1 - t0\n",
    "        t_tot = 0\n",
    "        dt_arr = []\n",
    "        while t_tot < torch.min(dt):\n",
    "            t_append = torch.min(torch.ones_like(dt)*self.max_step, dt - t_tot)\n",
    "            dt_arr.append(t_append)\n",
    "            t_tot += self.max_step\n",
    "        dt_arr = torch.cat(dt_arr, dim=1)\n",
    "        dt_arr = dt_arr[:,np.random.permutation(dt_arr.shape[1])]\n",
    "        dt_arr = torch.tensor(dt_arr).to(self.device)\n",
    "\n",
    "        for i in range(dt_arr.shape[1]):\n",
    "            y0 = self.RK4(y0, L0, dt_arr[:,[i]])\n",
    "\n",
    "        return y0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 400)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_L.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kp = 5.9055e3 #/ 800\n",
    "Ki = 3.0382 #/ 1\n",
    "Kd = 0\n",
    "Kbc = 0.0\n",
    "\n",
    "pid_par = (Kp, Ki, Kd, Kbc)\n",
    "\n",
    "# Create Dataset\n",
    "time_forecast = 10000\n",
    "# train_dataset = Dataset(training_t[:3,:], training_y[:3,:,:], training_L[:3,:], time_forecast=time_forecast)\n",
    "# val_dataset = Dataset(training_t[[3],:], training_y[[3],:,:], training_L[[3],:], time_forecast=time_forecast)\n",
    "\n",
    "dataset = Dataset(training_t, training_y, training_L, training_sp, time_forecast=time_forecast)\n",
    "\n",
    "\n",
    "# Min/Max Normalization\n",
    "\n",
    "\n",
    "# Train Validation Split\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Define the dataloader\n",
    "batch_size = 10000\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Create the model\n",
    "input_size = 5\n",
    "hidden_size = 64\n",
    "output_size = 4\n",
    "\n",
    "max_step = 0.04384\n",
    "\n",
    "model = Model(input_size, hidden_size, output_size, device, max_step, pid_par).to(device).to(torch.float64)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr= 1e-2)\n",
    "\n",
    "# Define the lr scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=50, verbose=True)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000, eta_min=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training loop\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, lr_scheduler=None, autoreg=0, L_autoreg=0):\n",
    "    size = len(dataloader.dataset)\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for batch, (t0, t1, L0, L1, y0, y1, sp) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred_y, pred_L = model(t0, t1, L0, sp, y0, autoreg, L_autoreg)\n",
    "\n",
    "        # Scale the predictions\n",
    "        y1 = y1.to(device) / (torch.tensor([2000,300,500,1])).to(device).unsqueeze(0)\n",
    "        pred_y = pred_y / (torch.tensor([2000,300,500,1])).to(device).unsqueeze(0)\n",
    "        L0 = L0.to(device) / (torch.tensor([800])).to(device).unsqueeze(0)\n",
    "        pred_L = pred_L / (torch.tensor([800])).to(device).unsqueeze(0)\n",
    "\n",
    "        loss_ar_y = loss_fn(pred_y, y1)\n",
    "        loss_ar_L = loss_fn(pred_L, L0)\n",
    "\n",
    "        pred_y, pred_L = model(t0, t1, L0, sp, y0, 0, L_autoreg)\n",
    "\n",
    "        # Scale the predictions\n",
    "        y1 = y1.to(device) / (torch.tensor([2000,300,500,1])).to(device).unsqueeze(0)\n",
    "        pred_y = pred_y / (torch.tensor([2000,300,500,1])).to(device).unsqueeze(0)\n",
    "        pred_L = pred_L / (torch.tensor([800])).to(device).unsqueeze(0)\n",
    "\n",
    "        loss_tf_y = loss_fn(pred_y, y1)\n",
    "        loss_tf_L = loss_fn(pred_L, L0)\n",
    "        # assert False\n",
    "\n",
    "        # loss = (torch.log10(loss_ar_y+loss_ar_L) + torch.log10(loss_tf_y+loss_tf_L))\n",
    "        loss = torch.log10(loss_ar_y) + torch.log10(loss_tf_y)\n",
    "        if loss_ar_L > 1e-6:\n",
    "            loss += torch.log10(loss_ar_L)\n",
    "        if loss_tf_L > 1e-6:\n",
    "            loss += torch.log10(loss_tf_L)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient Clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # if batch % 100 == 0:\n",
    "        #     loss, current = loss.item(), batch * len(t0)\n",
    "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    \n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step(train_loss)\n",
    "        # lr_scheduler.step()\n",
    "\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    # lr = lr_scheduler.get_last_lr()\n",
    "\n",
    "    return train_loss , lr\n",
    "\n",
    "\n",
    "def val_loop(dataloader, model, loss_fn, autoreg=1, L_autoreg=0):\n",
    "    size = len(dataloader.dataset)\n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for t0, t1, L0, L1, y0, y1, sp in dataloader:\n",
    "            pred_y, pred_L = model(t0, t1, L0, sp, y0, autoreg, L_autoreg)\n",
    "\n",
    "            # Scale the predictions\n",
    "            y1 = y1.to(device) / (torch.tensor([2000,300,500,1])).to(device).unsqueeze(0)\n",
    "            pred_y = pred_y / (torch.tensor([2000,300,500,1])).to(device).unsqueeze(0)\n",
    "            L0 = L0.to(device) / (torch.tensor([800])).to(device).unsqueeze(0)\n",
    "            pred_L = pred_L / (torch.tensor([800])).to(device).unsqueeze(0)\n",
    "\n",
    "            loss_y = loss_fn(pred_y, y1)\n",
    "            loss_L = loss_fn(pred_L, L0)\n",
    "                \n",
    "            # val_loss += (torch.log10(loss_y + loss_L)).item()\n",
    "            val_loss += (torch.log10(loss_y)).item()\n",
    "    \n",
    "    return val_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "589ee63a05324e1f809668e8da5e1eb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00128: reducing learning rate of group 0 to 5.0000e-03.\n",
      "Epoch 00234: reducing learning rate of group 0 to 2.5000e-03.\n",
      "Epoch 00285: reducing learning rate of group 0 to 1.2500e-03.\n",
      "Epoch 00336: reducing learning rate of group 0 to 6.2500e-04.\n",
      "Epoch 00387: reducing learning rate of group 0 to 3.1250e-04.\n",
      "Epoch 00438: reducing learning rate of group 0 to 1.5625e-04.\n"
     ]
    }
   ],
   "source": [
    "trainnew = True\n",
    "\n",
    "if trainnew:\n",
    "\n",
    "    # Train the model\n",
    "    tf_epochs = 100\n",
    "    transition_epochs = 1000\n",
    "    autoreg_epochs = 1000\n",
    "    epochs = tf_epochs + transition_epochs + autoreg_epochs\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    lr_list = []\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "\n",
    "    starting_ar = 0.9\n",
    "    final_ar = 0.999\n",
    "\n",
    "    starting_L_ar = 0.9\n",
    "    final_L_ar = 0.99\n",
    "\n",
    "    pbar = tqdm(range(epochs))\n",
    "    lr_scheduler = None\n",
    "\n",
    "    for t in pbar:\n",
    "        if t < tf_epochs:\n",
    "            autoreg = starting_ar\n",
    "            L_autoreg = starting_L_ar\n",
    "        elif t < tf_epochs + transition_epochs:\n",
    "            autoreg = starting_ar + (final_ar - starting_ar) * (t - tf_epochs) / transition_epochs\n",
    "            L_autoreg = starting_L_ar + (final_L_ar - starting_L_ar) * (t - tf_epochs) / transition_epochs\n",
    "        else:\n",
    "            autoreg = final_ar\n",
    "            L_autoreg = final_L_ar\n",
    "            if lr_scheduler is None:\n",
    "                lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=50, verbose=True)\n",
    "                # lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000, eta_min=1e-6)\n",
    "                for g in optimizer.param_groups:\n",
    "                    g['lr'] = 1e-2\n",
    "\n",
    "        train_loss, lr = train_loop(train_dataloader, model, loss_fn, optimizer, lr_scheduler, autoreg, L_autoreg)\n",
    "        val_loss = val_loop(val_dataloader, model, loss_fn, 1, 0)\n",
    "\n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "        lr_list.append(lr)\n",
    "\n",
    "        pbar.set_description(f\"Epoch {t+1}\")\n",
    "        pbar.set_postfix(train_loss=train_loss, val_loss=val_loss, lr=lr, autoreg=autoreg, L_autoreg=L_autoreg)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            path = \"/home/smalani/controlledLearning/trained_models/\"\n",
    "            torch.save(model, path + \"modeltrain_controller2_longtraj_best.pt\")\n",
    "\n",
    "    torch.save(model, path + \"modeltrain_controller2_longtraj.pt\")\n",
    "            \n",
    "        # pbar.set_postfix(train_loss=train_loss, val_loss=val_loss, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss\n",
    "if trainnew:\n",
    "    fig = plt.figure(figsize=(5, 4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(train_loss_list, label='Train Loss')\n",
    "    ax.plot(val_loss_list, label='Validation Loss')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Loss vs Epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model and test\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "path = \"/home/smalani/controlledLearning/trained_models/\"\n",
    "model = torch.load(path + \"modeltrain_controller2_longtraj_best.pt\", map_location=\"cpu\")\n",
    "model.device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Performance on training data\n",
    "\n",
    "def learned_model_ode(t, y0, L0):\n",
    "    L0 = torch.tensor(L0).to(device)\n",
    "    y0 = torch.tensor(y0).to(device)\n",
    "    if len(y0.shape) == 1:\n",
    "        y0 = y0.unsqueeze(0)\n",
    "    while len(y0.shape) > len(L0.shape):\n",
    "        L0 = L0.unsqueeze(-1)\n",
    "    dydt = model.ODEs(y0, L0)\n",
    "\n",
    "    return dydt.cpu().detach().numpy().squeeze()\n",
    "    \n",
    "\n",
    "y0_in = dataset.y0.reshape((-1, 4))\n",
    "L0_in = dataset.L0.reshape((-1, 1))\n",
    "\n",
    "true_dydt = khammash_repro.ode_fun(0, y0_in.T, L0_in.T).T * 60\n",
    "learned_dydt = learned_model_ode(0, y0_in, L0_in)\n",
    "\n",
    "fig = plt.figure(figsize=(5, 4))\n",
    "ax = fig.add_subplot(221)\n",
    "ax.scatter(true_dydt[:,0], learned_dydt[:,0], s=1)\n",
    "ax.plot([np.min(true_dydt[:,0]), np.max(true_dydt[:,0])], [np.min(true_dydt[:,0]), np.max(true_dydt[:,0])], 'k--')\n",
    "\n",
    "ax = fig.add_subplot(222)\n",
    "ax.scatter(true_dydt[:,1], learned_dydt[:,1], s=1)\n",
    "ax.plot([np.min(true_dydt[:,1]), np.max(true_dydt[:,1])], [np.min(true_dydt[:,1]), np.max(true_dydt[:,1])], 'k--')\n",
    "\n",
    "ax = fig.add_subplot(223)\n",
    "ax.scatter(true_dydt[:,2], learned_dydt[:,2], s=1)\n",
    "ax.plot([np.min(true_dydt[:,2]), np.max(true_dydt[:,2])], [np.min(true_dydt[:,2]), np.max(true_dydt[:,2])], 'k--')\n",
    "\n",
    "ax = fig.add_subplot(224)\n",
    "ax.scatter(true_dydt[:,3], learned_dydt[:,3], s=1)\n",
    "ax.plot([np.min(true_dydt[:,3]), np.max(true_dydt[:,3])], [np.min(true_dydt[:,3]), np.max(true_dydt[:,3])], 'k--')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "def learned_model_ode(t, y0, L0):\n",
    "    L0 = torch.tensor(L0).to(device)\n",
    "    y0 = torch.tensor(y0).to(device)\n",
    "    if len(y0.shape) == 1:\n",
    "        y0 = y0.unsqueeze(0)\n",
    "    while len(y0.shape) > len(L0.shape):\n",
    "        L0 = L0.unsqueeze(-1)\n",
    "    dydt = model.ODEs(y0, L0)\n",
    "\n",
    "    return dydt.cpu().detach().numpy().squeeze()\n",
    "\n",
    "# x_init = khammash_repro.get_init_cond(L0=100)\n",
    "# x_init[-1] = 0.4\n",
    "x_init = dataset.y0[4,0,:]\n",
    "L0 = dataset.L0[4,0]\n",
    "L0 = 100\n",
    "t_span = np.array([0, 100])\n",
    "t_eval = np.linspace(t_span[0], t_span[1], 1000)\n",
    "\n",
    "sol_true = solve_ivp(khammash_repro.ode_fun, t_span*60, x_init, args=(L0,), t_eval=t_eval*60, method='BDF', rtol=1e-10, atol=1e-10, first_step=1e-10)\n",
    "sol_learned = solve_ivp(learned_model_ode, t_span, x_init, args=(L0,), t_eval=t_eval, method='BDF', rtol=1e-10, atol=1e-10, first_step=1e-10)\n",
    "\n",
    "print(learned_model_ode(0, x_init, L0))\n",
    "print(khammash_repro.ode_fun(0, x_init, L0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.y0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 4))\n",
    "for i in range(4):\n",
    "    ax = fig.add_subplot(2,2,i+1)\n",
    "    ax.plot(sol_true.t/60, sol_true.y[i,:], 'k-', label='True')\n",
    "    ax.plot(sol_learned.t, sol_learned.y[i,:], 'r--', label='Learned')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Species {}'.format(i+1))\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fsolve\n",
    "\n",
    "def get_steady_state_fsolve(x, L):\n",
    "    input = np.hstack((x, np.array([0.5])))\n",
    "    ode_out = learned_model_ode(0, input, L)\n",
    "    return ode_out[:-1]\n",
    "\n",
    "def get_steady_state(L):\n",
    "    x_guess = [1500, 100, 1000]\n",
    "    ss, infodict, ier, mesg = fsolve(get_steady_state_fsolve, x_guess, args=(L,), xtol=1e-10, full_output=True, factor=0.01)\n",
    "    return ss\n",
    "\n",
    "def get_steady_state_fsolve_full(x, L):\n",
    "    input = x\n",
    "    ode_out = learned_model_ode(0, input, L)\n",
    "    return ode_out\n",
    "\n",
    "def get_steady_state_full(L):\n",
    "    x_init = khammash_repro.get_init_cond(L0=L)\n",
    "    t_span = np.array([0, 1000])\n",
    "    t_eval = np.linspace(t_span[0], t_span[1], 1000)\n",
    "    sol = solve_ivp(learned_model_ode, t_span, x_init, args=(L,), t_eval=t_eval, method='BDF', rtol=1e-3, atol=1e-6, first_step=1e-10)\n",
    "\n",
    "    x_guess = sol.y[:,-1]\n",
    "\n",
    "    ss, infodict, ier, mesg = fsolve(get_steady_state_fsolve_full, x_guess, args=(L,), xtol=1e-10, full_output=True, factor=0.01)\n",
    "    return ss\n",
    "\n",
    "def L_equal_fsolve2(x):\n",
    "    init_cond = get_steady_state(x)\n",
    "    init_cond = np.hstack((init_cond, np.array([0.3])))\n",
    "    ode_out = learned_model_ode(0, init_cond, x)\n",
    "    return ode_out[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_eq, infodict, ier, mesg = fsolve(L_equal_fsolve2, 300, xtol=1e-10, full_output=True, factor=1)\n",
    "print(mesg)\n",
    "print(\"Both strains grow equally well at L = \" + str(L_eq[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.sort(np.concatenate([np.linspace(0,800,30), L_eq]))\n",
    "\n",
    "phi_steady = []\n",
    "for L0 in L:\n",
    "    init_cond = get_steady_state_full(L=L0)\n",
    "    phi_steady.append(init_cond[-1])\n",
    "\n",
    "phi_steady = np.array(phi_steady)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_eq_arr = np.ones(20) * L_eq\n",
    "phi_eq_arr = np.linspace(0, 1, len(L_eq_arr))\n",
    "\n",
    "L_plot_steady = np.sort(np.concatenate([L, L_eq_arr]))\n",
    "phi_plot_steady = np.sort(np.concatenate([phi_steady, phi_eq_arr]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_vec_arr = np.linspace(0, 800, 20)\n",
    "phi_vec_arr = np.linspace(0, np.max(phi_steady), 10)\n",
    "\n",
    "dphi_vec = np.zeros((len(L_vec_arr), len(phi_vec_arr)))\n",
    "\n",
    "for i, L0 in enumerate(L_vec_arr):\n",
    "    for j, phi0 in enumerate(phi_vec_arr):\n",
    "        init_cond = khammash_repro.get_init_cond(L0=L0)\n",
    "        init_cond[-1] = phi0\n",
    "        # init_cond = np.hstack((init_cond, np.array([phi0])))\n",
    "        ode_out = learned_model_ode(0, init_cond, L0)\n",
    "        dphi_vec[i,j] = ode_out[-1]\n",
    "\n",
    "# print(dphi_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symlog(x):\n",
    "    \"\"\" Returns the symmetric log10 value \"\"\"\n",
    "    return np.sign(x) * np.log10(np.abs(x)+1)\n",
    "\n",
    "def transform(u, v):\n",
    "    arrow_lengths = np.sqrt(u*u + v*v)\n",
    "    # len_adjust_factor = np.log10(arrow_lengths + 1) / arrow_lengths\n",
    "    len_adjust_factor = np.log10(np.abs(arrow_lengths+1)) / arrow_lengths\n",
    "    return u*len_adjust_factor, v*len_adjust_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shiftedColorMap(cmap, start=0, midpoint=0.5, stop=1.0, name='shiftedcmap'):\n",
    "    '''\n",
    "    Function to offset the \"center\" of a colormap. Useful for\n",
    "    data with a negative min and positive max and you want the\n",
    "    middle of the colormap's dynamic range to be at zero.\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "      cmap : The matplotlib colormap to be altered\n",
    "      start : Offset from lowest point in the colormap's range.\n",
    "          Defaults to 0.0 (no lower offset). Should be between\n",
    "          0.0 and `midpoint`.\n",
    "      midpoint : The new center of the colormap. Defaults to \n",
    "          0.5 (no shift). Should be between 0.0 and 1.0. In\n",
    "          general, this should be  1 - vmax / (vmax + abs(vmin))\n",
    "          For example if your data range from -15.0 to +5.0 and\n",
    "          you want the center of the colormap at 0.0, `midpoint`\n",
    "          should be set to  1 - 5/(5 + 15)) or 0.75\n",
    "      stop : Offset from highest point in the colormap's range.\n",
    "          Defaults to 1.0 (no upper offset). Should be between\n",
    "          `midpoint` and 1.0.\n",
    "    '''\n",
    "    cdict = {\n",
    "        'red': [],\n",
    "        'green': [],\n",
    "        'blue': [],\n",
    "        'alpha': []\n",
    "    }\n",
    "\n",
    "    # regular index to compute the colors\n",
    "    reg_index = np.linspace(start, stop, 257)\n",
    "\n",
    "    # shifted index to match the data\n",
    "    shift_index = np.hstack([\n",
    "        np.linspace(0.0, midpoint, 128, endpoint=False), \n",
    "        np.linspace(midpoint, 1.0, 129, endpoint=True)\n",
    "    ])\n",
    "\n",
    "    for ri, si in zip(reg_index, shift_index):\n",
    "        r, g, b, a = cmap(ri)\n",
    "\n",
    "        cdict['red'].append((si, r, r))\n",
    "        cdict['green'].append((si, g, g))\n",
    "        cdict['blue'].append((si, b, b))\n",
    "        cdict['alpha'].append((si, a, a))\n",
    "\n",
    "    newcmap = matplotlib.colors.LinearSegmentedColormap(name, cdict)\n",
    "    plt.register_cmap(cmap=newcmap)\n",
    "\n",
    "    return newcmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(L_vec_arr.shape)\n",
    "print(phi_vec_arr.shape)\n",
    "print(dphi_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "newcmap = shiftedColorMap(matplotlib.cm.coolwarm, midpoint=1-np.max(dphi_vec)/(np.max(dphi_vec)-np.min(dphi_vec)))\n",
    "angles=np.arctan2(dphi_vec.T,np.zeros_like(dphi_vec.T))*180.0/np.pi\n",
    "U2, V2 = transform(np.zeros_like(dphi_vec.T), dphi_vec.T)\n",
    "\n",
    "L_vec_mesh, phi_vec_mesh = np.meshgrid(L_vec_arr, phi_vec_arr)\n",
    "cax = ax.contourf(L_vec_arr, phi_vec_arr, dphi_vec.T, 100, cmap= newcmap)\n",
    "\n",
    "ax.quiver(L_vec_mesh, phi_vec_mesh, U2, V2)\n",
    "# ax.quiver(L_vec_mesh, phi_vec_mesh, np.zeros_like(dphi_vec.T), dphi_vec.T)\n",
    "\n",
    "ax.plot(L_plot_steady, phi_plot_steady, linewidth=3, color='green')\n",
    "ax.set_xlabel('L', fontsize=16)\n",
    "ax.set_ylabel(r'$\\phi$', fontsize=16)\n",
    "ax.set_title(r'$d\\phi/dt$', fontsize=20)\n",
    "# ax.set_ylim([-0.05, 1.05])\n",
    "# ax.set_yticks([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])\n",
    "ax.tick_params(axis='both', which='major', labelsize=16, width=2, length=6, direction='in')\n",
    "ax.set_xticks([0, 80, 160, 240, 320, 400, 480, 560, 640, 720, 800])\n",
    "# fig.colorbar(cax, ticks=[-0.005, -0.004, -0.003, -0.002, -0.001, 0.000, 0.001], label=r'$d\\phi/dt$')\n",
    "fig.colorbar(cax, label=r'$d\\phi/dt$')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
